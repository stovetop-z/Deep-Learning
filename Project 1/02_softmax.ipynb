{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Softmax Implementation for Linear Classifier\n",
    "\n",
    "\n",
    "This project will teach you how to implement softmax activation and use PyTorch's automatic differentiation for training a linear classifier. You will:\n",
    "\n",
    "1. **Implement softmax activation function** for multi-class classification with numerical stability\n",
    "2. **Use PyTorch's automatic differentiation** instead of manual gradient computation\n",
    "3. **Train a linear classifier** using default hyperparameters (lr=0.001, batch_size=32)\n",
    "4. **Test the trained classifier on MNIST dataset** and achieve >90% accuracy\n",
    "5. **Load pre-saved weights** and compare with the original linear classifier from 01_linear_feature.ipynb\n",
    "\n",
    "**Important**: This notebook focuses on understanding softmax implementation and PyTorch's automatic differentiation. We will use PyTorch's cross-entropy loss function and optimizer for training.\n",
    "\n",
    "***NOTE:***\n",
    "When filling in the code, please REMOVE the `pass` statement.\n",
    "DO NOT remove the TODO coding highlight in your submission.\n",
    "\n",
    "**Grading Criteria**:\n",
    "- Test accuracy must be >90% (60% points)\n",
    "- Code quality and implementation (10% points)\n",
    "- Analysis and answers to questions (30% points)\n",
    "- **Total: 5 points (50% of project score)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google Colab Setup (Comment out for local computer running)\n",
    "################################################################################\n",
    "# Uncomment and set the path to your project folder in Google Drive\n",
    "################################################################################\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# FOLDERNAME = 'cpsc8430/assignments/project1/'\n",
    "# assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# # Now that we've mounted your Drive, this ensures that\n",
    "# # the Python interpreter of the Colab VM can load\n",
    "# # python files from within it.\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load and Preprocess MNIST Dataset\n",
    "\n",
    "First, let's load the MNIST dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='cpsc8430/datasets/MNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='cpsc8430/datasets/MNIST',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Visualize some training examples\n",
    "plt.figure(figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(train_dataset[i][0].squeeze(), cmap='gray')\n",
    "    plt.title(f'Label: {train_dataset[i][1]}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement Softmax Function\n",
    "\n",
    "**TODO: Implement the softmax function from scratch**\n",
    "\n",
    "The softmax function converts raw scores (logits) into probabilities:\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}$$\n",
    "\n",
    "**Important Hint for Numerical Stability**:\n",
    "To avoid numerical overflow when computing exponentials, subtract the maximum value from each input before applying exp:\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max_j x_j}}{\\sum_{j=1}^{K} e^{x_j - \\max_j x_j}}$$\n",
    "\n",
    "This ensures that the largest exponent is 0, preventing overflow while maintaining the same mathematical result.\n",
    "\n",
    "**Requirements**:\n",
    "- Handle numerical stability (subtract max value before exp)\n",
    "- Return probabilities that sum to 1\n",
    "- Work with both 1D and 2D inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities for input scores\n",
    "    \n",
    "    Args:\n",
    "        x: Input scores tensor of shape (batch_size, num_classes) or (num_classes,)\n",
    "    \n",
    "    Returns:\n",
    "        Softmax probabilities tensor of same shape\n",
    "    \"\"\"\n",
    "    ################################################################################\n",
    "    # TODO: Implement softmax function\n",
    "    # Hint: Use torch.exp() and torch.sum()\n",
    "    # Hint: For numerical stability, subtract the max value before computing exp\n",
    "    # Hint: The equation is: softmax(x_i) = exp(x_i - max_j x_j) / sum_j exp(x_j - max_j x_j)\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                              END OF YOUR CODE                                #\n",
    "    ################################################################################\n",
    "\n",
    "\n",
    "# Test your softmax implementation\n",
    "test_scores = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "test_probs = softmax(test_scores)\n",
    "print(f\"Test scores:\\n{test_scores}\")\n",
    "print(f\"Softmax probabilities:\\n{test_probs}\")\n",
    "print(f\"Probabilities sum to 1: {torch.allclose(torch.sum(test_probs, dim=1), torch.ones(2))}\")\n",
    "\n",
    "# Test 1D input as well\n",
    "test_scores_1d = torch.tensor([1.0, 2.0, 3.0])\n",
    "test_probs_1d = softmax(test_scores_1d)\n",
    "print(f\"\\n1D test scores: {test_scores_1d}\")\n",
    "print(f\"1D softmax probabilities: {test_probs_1d}\")\n",
    "print(f\"1D probabilities sum to 1: {torch.allclose(torch.sum(test_probs_1d), torch.tensor(1.0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Linear Classifier Forward Pass\n",
    "\n",
    "**TODO: Implement the forward pass of the linear classifier**\n",
    "\n",
    "The forward pass computes: $f(x) = \\text{softmax}(xW^T + b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_classifier_forward(x, weights, bias):\n",
    "    \"\"\"\n",
    "    Forward pass of linear classifier (returns logits, not probabilities)\n",
    "    \n",
    "    Args:\n",
    "        x: Input features, shape (batch_size, input_features)\n",
    "        weights: Weight matrix, shape (num_classes, input_features)\n",
    "        bias: Bias vector, shape (num_classes,)\n",
    "    \n",
    "    Returns:\n",
    "        Logits (raw output class scores), shape (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    ################################################################################\n",
    "    # TODO: Implement forward pass\n",
    "    # Hint: Compute linear transformation: x * weights^T + bias\n",
    "    # Return the logits (do NOT apply softmax here)\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                              END OF YOUR CODE                                #\n",
    "    ################################################################################\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Test your forward pass implementation\n",
    "batch_size = 4\n",
    "input_features = 784\n",
    "num_classes = 10\n",
    "\n",
    "test_x = torch.randn(batch_size, input_features)\n",
    "test_weights = torch.randn(num_classes, input_features)\n",
    "test_bias = torch.randn(num_classes)\n",
    "\n",
    "test_output = linear_classifier_forward(test_x, test_weights, test_bias)\n",
    "print(f\"Input shape: {test_x.shape}\")\n",
    "print(f\"Weights shape: {test_weights.shape}\")\n",
    "print(f\"Bias shape: {test_bias.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Output probabilities sum to 1: {torch.allclose(torch.sum(test_output, dim=1), torch.ones(batch_size))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train Linear Classifier Using PyTorch\n",
    "\n",
    "\n",
    "**TODO: Train the linear classifier using PyTorch's automatic differentiation**\n",
    "\n",
    "Instead of implementing gradient descent manually, we'll use PyTorch's built-in optimizer and automatic differentiation. This will use the default learning rate of 0.001 and batch size of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_classifier(train_loader, num_epochs=10):\n",
    "    \"\"\"\n",
    "    Train linear classifier using PyTorch's automatic differentiation\n",
    "    \n",
    "    Args:\n",
    "        train_loader: DataLoader for training data\n",
    "        num_epochs: Number of training epochs (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        Trained weights, bias, and training history\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    input_features = 784  # 28x28 flattened\n",
    "    num_classes = 10\n",
    "\n",
    "    weights = None\n",
    "    bias = None\n",
    "\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    criterion = None\n",
    "    optimizer = None\n",
    "    \n",
    "    \n",
    "    ################################################################################\n",
    "    # TODO: Initialize weights and bias, set requires_grad=True, and set up loss function and optimizer\n",
    "    # Hint: \n",
    "    #   - Use torch.randn() or torch.zeros() to initialize weights and bias\n",
    "    #   - Use weights.requires_grad_(True) and bias.requires_grad_(True)\n",
    "    #   - Use nn.CrossEntropyLoss() and torch.optim.SGD() with lr=0.001\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ################################################################################\n",
    "    #                              END OF YOUR CODE                                #\n",
    "    ################################################################################\n",
    "    \n",
    "\n",
    "    print(f\"Starting training with {num_epochs} epochs, lr=0.001, batch_size=32\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            # Flatten input data\n",
    "            data = data.view(-1, input_features)\n",
    "            \n",
    "            ################################################################################\n",
    "            # TODO: Forward pass, compute loss, and perform backward pass and optimization\n",
    "            # Hint: \n",
    "            #   - Use your linear_classifier_forward function for the forward pass\n",
    "            #   - CrossEntropyLoss expects logits, not probabilities (compute logits before softmax)\n",
    "            #   - optimizer.zero_grad(), loss.backward(), optimizer.step()\n",
    "            ################################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            \n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            ################################################################################\n",
    "            #                              END OF YOUR CODE                                #\n",
    "            ################################################################################\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Compute epoch statistics\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return weights, bias, train_losses, train_accuracies\n",
    "\n",
    "# Create data loader with default batch size of 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Test training function\n",
    "print(\"Testing training function...\")\n",
    "weights, bias, losses, accuracies = train_linear_classifier(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Training Progress\n",
    "\n",
    "Plot the training curves to see how the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final training accuracy: {accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualize Learned Templates\n",
    "\n",
    "**NEW SECTION: Visualize the learned weight templates for each digit class**\n",
    "\n",
    "The weights of a linear classifier can be interpreted as learned templates for each class. Let's visualize what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned templates\n",
    "from PIL import Image\n",
    "\n",
    "def visualize_learned_templates(weights, save_path='all_weights_combined_learned.png'):\n",
    "    \"\"\"\n",
    "    Visualize the learned weight templates for each digit class\n",
    "    \n",
    "    Args:\n",
    "        weights: Trained weight matrix, shape (num_classes, input_features)\n",
    "        save_path: Path to save the combined visualization\n",
    "    \"\"\"\n",
    "    # Reshape weights to 28x28 images\n",
    "    num_classes, input_features = weights.shape\n",
    "    assert input_features == 784, f\"Expected 784 features, got {input_features}\"\n",
    "    \n",
    "    # Create a figure with 2x5 subplots for the 10 digits\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    # Normalize weights for better visualization\n",
    "    weights_normalized = weights.clone()\n",
    "    for i in range(num_classes):\n",
    "        # Normalize each class template to [0, 1] range\n",
    "        w_min = weights[i].min()\n",
    "        w_max = weights[i].max()\n",
    "        if w_max > w_min:\n",
    "            weights_normalized[i] = (weights[i] - w_min) / (w_max - w_min)\n",
    "    \n",
    "    # Plot each digit template\n",
    "    for i in range(num_classes):\n",
    "        # Reshape to 28x28\n",
    "        template = weights_normalized[i].detach().view(28, 28).numpy()\n",
    "        \n",
    "        # Plot template\n",
    "        axes[i].imshow(template, cmap='gray')\n",
    "        axes[i].set_title(f'Learned Template for Digit {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create combined visualization (similar to 01_linear_feature.ipynb)\n",
    "    # Create a 10x1 grid layout (single column)\n",
    "    combined_img = np.zeros((28 * 10, 28))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        row = i  # Each digit gets its own row\n",
    "\n",
    "        template = weights_normalized[i].detach().view(28, 28).numpy()\n",
    "        combined_img[row*28:(row+1)*28, :] = template\n",
    "    \n",
    "    # Display combined image\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(combined_img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()   \n",
    "    \n",
    "    # Save combined image directly as raw array\n",
    "    combined_img_normalized = ((combined_img - combined_img.min()) / (combined_img.max() - combined_img.min()) * 255).astype(np.uint8)\n",
    "    img = Image.fromarray(combined_img_normalized, mode='L')\n",
    "    img.save(save_path)\n",
    "    \n",
    "    print(f\"‚úÖ Learned templates visualization saved to: {save_path}\")\n",
    "    return combined_img\n",
    "\n",
    "# Visualize the learned templates\n",
    "print(\"Visualizing learned templates...\")\n",
    "learned_templates = visualize_learned_templates(weights)\n",
    "\n",
    "# Also show individual templates with more detail\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    template = weights[i].detach().view(28, 28).numpy()\n",
    "    plt.imshow(template, cmap='gray')\n",
    "    plt.title(f'Digit {i} Template (Raw Weights)')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Test on MNIST Test Set\n",
    "\n",
    "Evaluate the trained model on the test set to get the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_loader, weights, bias):\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on test set\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            # Flatten input data\n",
    "            data = data.view(-1, 784)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = linear_classifier_forward(data, weights, bias)\n",
    "            \n",
    "            # Get predictions\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            \n",
    "            # Update statistics\n",
    "            \n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Evaluate model\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "test_accuracy = evaluate_model(test_loader, weights, bias)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# IMPORTANT: This accuracy should be greater than 90% for full credit\n",
    "# assert test_accuracy > 90.0, f\"Test accuracy {test_accuracy:.2f}% is below the required threshold of 90%\"\n",
    "# print(f\"‚úÖ Test accuracy {test_accuracy:.2f}% meets the requirement of >90%!\")\n",
    "\n",
    "# Store the result for grading\n",
    "test_accuracy_result = test_accuracy\n",
    "\n",
    "# Test metadata for Gradescope auto-grading\n",
    "# This cell will be automatically executed and evaluated\n",
    "print(f\"\\nüéØ Final Test Result: {test_accuracy_result:.2f}%\")\n",
    "print(f\"üìä Grading Status: {'‚úÖ PASSED' if test_accuracy_result > 90.0 else '‚ùå FAILED'}\")\n",
    "print(f\"üéì Points Earned: {60 if test_accuracy_result > 90.0 else 0}/60 for accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Load Pre-saved Weights and Compare\n",
    "\n",
    "**NEW SECTION: Load pre-saved weight images and apply them to the linear classifier**\n",
    "\n",
    "In this section, we'll load the pre-saved weight images (like `all_digits_combined_learned.png`) and use them as weights for the linear classifier. We'll then compare the accuracy with the original implementation from 01_linear_feature.ipynb.\n",
    "\n",
    "**Key Features:**\n",
    "1. **Load images** using the same approach as 01_linear_feature.ipynb\n",
    "2. **Apply eta adjustment** to modify the loaded images\n",
    "3. **Create weight matrix** [10, 784] from normalized images\n",
    "4. **Test accuracy** using the linear classifier evaluation function\n",
    "5. **Compare results** across different eta values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear classifier functions from 01_linear_feature.ipynb\n",
    "from cpsc8430.classifiers import (\n",
    "    load_and_preprocess_image,\n",
    "    create_weight_matrix,\n",
    "    create_mnist_test_loader,\n",
    "    evaluate_linear_classifier,\n",
    "    create_random_bias\n",
    ")\n",
    "\n",
    "# Load weights from the saved image with different eta values\n",
    "print(\"Loading weights from saved image...\")\n",
    "\n",
    "################################################################################\n",
    "# TODO: Try different values of eta (e.g., 0.0, 0.25, 0.5, 0.75, 1.0) and observe how the test accuracy changes.\n",
    "#       You can loop over several eta values and report the results for each.\n",
    "eta = 0.5\n",
    "################################################################################\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Testing with eta = {eta}\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "try:\n",
    "    # Load and preprocess image using eta (similar eta in 01_linear_feature.ipynb)\n",
    "    img_normalized = load_and_preprocess_image('all_weights_combined_learned.png', eta)\n",
    "    \n",
    "    # Create weight matrix [10, 784] from the normalized image\n",
    "    weight_matrix = create_weight_matrix(img_normalized)\n",
    "    \n",
    "    # Create random bias\n",
    "    bias = torch.randn(10)\n",
    "    \n",
    "    # Create test loader\n",
    "    test_loader = create_mnist_test_loader()\n",
    "    \n",
    "    # Evaluation function\n",
    "    def evaluate():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                # Flatten the images\n",
    "                images = images.view(-1, 784)\n",
    "                \n",
    "                # Forward pass: compute scores\n",
    "                scores = torch.mm(images, weight_matrix.t()) + bias\n",
    "                \n",
    "                # Get predictions\n",
    "                _, predicted = torch.max(scores, 1)\n",
    "                \n",
    "                # Update statistics\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return 100 * correct / total\n",
    "    \n",
    "    # Test the classifier\n",
    "    accuracy = evaluate()\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    # Visualize the loaded weights with current eta\n",
    "    print(f\"Visualizing weights with eta={eta}...\")\n",
    "    visualize_learned_templates(weight_matrix, f'loaded_weights_eta_{eta:.2f}.png')\n",
    "    \n",
    "    # Show the effect of eta on a sample digit template\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # For comparison, also load the original weights (eta=0.0)\n",
    "    img_normalized_orig = load_and_preprocess_image('all_weights_combined_learned.png', eta=0.0)\n",
    "    original_weights = create_weight_matrix(img_normalized_orig)\n",
    "    current_weights = weight_matrix\n",
    "    \n",
    "    # Show comparison for digit 0\n",
    "    plt.subplot(1, 3, 1)\n",
    "    template_orig = original_weights[0].detach().view(28, 28).numpy()\n",
    "    plt.imshow(template_orig, cmap='gray')\n",
    "    plt.title(f'Original Weights (eta=0.0)\\nDigit 0')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    template_current = current_weights[0].detach().view(28, 28).numpy()\n",
    "    plt.imshow(template_current, cmap='gray')\n",
    "    plt.title(f'Modified Weights (eta={eta})\\nDigit 0')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    difference = template_current - template_orig\n",
    "    plt.imshow(difference, cmap='RdBu', vmin=-0.5, vmax=0.5)\n",
    "    plt.title(f'Difference (eta={eta} - eta=0.0)\\nRed: Increased, Blue: Decreased')\n",
    "    plt.colorbar()\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Eta={eta} analysis completed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with eta={eta}: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Questions\n",
    "\n",
    "### Question 1\n",
    "What is the purpose of the softmax function in multi-class classification? How does it help with gradient descent?\n",
    "\n",
    "$\\color{blue}{\\textit{Your Answer:}}$\n",
    "\n",
    "\n",
    "\n",
    "### Question 2\n",
    "What is the significance of the cross-entropy loss function in classification problems? Why is it preferred over mean squared error?\n",
    "\n",
    "$\\color{blue}{\\textit{Your Answer:}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question 3\n",
    "**Analyze the learned templates visualization**\n",
    "\n",
    "Look at the learned weight templates for each digit class. What patterns do you observe? How do these templates relate to the actual digit shapes? Why might some templates look clearer than others? What does this tell us about how the linear classifier learns to distinguish between different digit classes?\n",
    "\n",
    "$\\color{blue}{\\textit{Your Answer:}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Question 4\n",
    "**Weight loading and comparison**\n",
    "\n",
    "When you load the pre-saved weights, 'all_weights_combined_learned.png' , from the image file, how does the testing accuracy compare to the direct trained weights (Part 7)?  What is the major reason to cause differences in performance? \n",
    "\n",
    "$\\color{blue}{\\textit{Your Answer:}}$\n",
    "\n",
    "### Question 5\n",
    "**Effect of Different Eta Values on Testing Accuracy**\n",
    "\n",
    "In Part 8, try different values of $\\eta$ from the set $\\{0, 0.25, 0.5, 0.75, 1\\}$ when loading the pre-saved weights from the image file. For each $\\eta$, report the testing accuracy you obtain.\n",
    "\n",
    "$\\color{blue}{\\textit{Your Answer:}}$\n",
    "\n",
    "- Testing accuracy for $\\eta=0$:\n",
    "- Testing accuracy for $\\eta=0.25$:\n",
    "- Testing accuracy for $\\eta=0.5$:\n",
    "- Testing accuracy for $\\eta=0.75$:\n",
    "- Testing accuracy for $\\eta=1$:\n",
    "\n",
    "How does the trend of testing accuracy as $\\eta$ changes compare to what you observed in 01_linear_feature.ipynb? Why do you think the trend is different in this notebook?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
